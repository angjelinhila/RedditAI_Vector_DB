{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /tmp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /tmp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.path.append('/tmp/nltk_data')\n",
    "nltk.download(\"punkt\", download_dir='/tmp/nltk_data')\n",
    "nltk.download(\"stopwords\", download_dir='/tmp/nltk_data')\n",
    "nltk.download(\"wordnet\", download_dir='/tmp/nltk_data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "RAW_DATA_DIR = \"/Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Raw\"\n",
    "PROCESSED_DATA_DIR = \"/Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure processed directory exists\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Load ColBERT tokenizer (for FAISS processing)\n",
    "colbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Stopwords for optional removal\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning: remove URLs, special characters, and extra spaces.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, tokenizer, max_length=512, stride=256):\n",
    "    \"\"\"Splits text into overlapping chunks to fit within model limits.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [tokens[i: i + max_length] for i in range(0, len(tokens), stride)]\n",
    "    return [tokenizer.convert_tokens_to_string(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1️⃣ Preprocessing for Database (BM25 + FAISS) ###\n",
    "def preprocess_for_db():\n",
    "    \"\"\"Preprocesses data for BM25 (Elasticsearch) and FAISS (ColBERT).\"\"\"\n",
    "    for filename in os.listdir(RAW_DATA_DIR):\n",
    "        if filename.endswith(\"_comments.json\") or filename.endswith(\"_submissions.json\"):\n",
    "            input_file = os.path.join(RAW_DATA_DIR, filename)\n",
    "            output_file = os.path.join(PROCESSED_DATA_DIR, filename.replace(\".json\", \"_cleaned.json\"))\n",
    "            \n",
    "            with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "                for line in infile:\n",
    "                    data = json.loads(line)\n",
    "                    \n",
    "                    # Process body or selftext depending on file type\n",
    "                    text_field = \"selftext\" if \"selftext\" in data else \"body\"\n",
    "                    data[text_field] = clean_text(data.get(text_field, \"\"))\n",
    "                    \n",
    "                    # FAISS-specific: Chunk tokenized text for ColBERT\n",
    "                    data[\"tokenized_body\"] = chunk_text(data[text_field], colbert_tokenizer)\n",
    "                    \n",
    "                    json.dump(data, outfile)\n",
    "                    outfile.write(\"\\n\")\n",
    "            print(f\"Database preprocessing complete: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1️⃣ Preprocessing for Database (BM25 + FAISS) ###\n",
    "def preprocess_for_db():\n",
    "    \"\"\"Preprocesses data for BM25 (Elasticsearch) and FAISS (ColBERT).\"\"\"\n",
    "    for filename in os.listdir(RAW_DATA_DIR):\n",
    "        if filename.endswith(\"_comments.json\") or filename.endswith(\"_submissions.json\"):\n",
    "            input_file = os.path.join(RAW_DATA_DIR, filename)\n",
    "            output_file = os.path.join(PROCESSED_DATA_DIR, filename.replace(\".json\", \"_cleaned.json\"))\n",
    "            \n",
    "            with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "                for line in infile:\n",
    "                    data = json.loads(line)\n",
    "                    \n",
    "                    # Process body or selftext depending on file type\n",
    "                    text_field = \"selftext\" if \"selftext\" in data else \"body\"\n",
    "                    data[text_field] = clean_text(data.get(text_field, \"\"))\n",
    "                    \n",
    "                    # FAISS-specific: tokenize for ColBERT\n",
    "                    data[\"tokenized_body\"] = colbert_tokenizer.tokenize(data[text_field])\n",
    "                    \n",
    "                    json.dump(data, outfile)\n",
    "                    outfile.write(\"\\n\")\n",
    "            print(f\"Database preprocessing complete: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (629 > 512). Running this sequence through the model will result in indexing errors\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x104018b60>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database preprocessing complete: /Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed/artificial_comments_cleaned.json\n",
      "Database preprocessing complete: /Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed/artificialinteligence_comments_cleaned.json\n",
      "Database preprocessing complete: /Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed/openai_comments_cleaned.json\n",
      "Database preprocessing complete: /Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed/artificialinteligence_submissions_cleaned.json\n",
      "Database preprocessing complete: /Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed/chatgpt_submissions_cleaned.json\n",
      "Database preprocessing complete: /Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed/transhumanism_submissions_cleaned.json\n",
      "Database preprocessing complete: /Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed/chatgpt_comments_cleaned.json\n",
      "Database preprocessing complete: /Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed/artificial_submissions_cleaned.json\n",
      "Database preprocessing complete: /Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed/singularity_comments_cleaned.json\n",
      "Database preprocessing complete: /Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed/openai_submissions_cleaned.json\n",
      "Database preprocessing complete: /Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed/singularity_submissions_cleaned.json\n",
      "Database preprocessing complete: /Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed/transhumannism_comments_cleaned.json\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Run preprocessing\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_for_db()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
