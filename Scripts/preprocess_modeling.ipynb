{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "RAW_DATA_DIR = \"/Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Raw\"\n",
    "PROCESSED_DATA_DIR = \"/Users/angjelin/Library/CloudStorage/Box-Box/Reddit Vector DB/Data/Processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure processed directory exists\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ColBERT tokenizer (for FAISS processing)\n",
    "colbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords for optional removal\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning: remove URLs, special characters, and extra spaces.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, tokenizer, max_length=512, stride=256):\n",
    "    \"\"\"Splits text into overlapping chunks to fit within model limits.\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = [tokens[i: i + max_length] for i in range(0, len(tokens), stride)]\n",
    "    return [tokenizer.convert_tokens_to_string(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2️⃣ Preprocessing for Modelling (Deep Learning) ###\n",
    "def preprocess_for_modelling():\n",
    "    \"\"\"Prepares text for machine learning: stopword removal, lemmatization, and tokenization.\"\"\"\n",
    "    for filename in os.listdir(RAW_DATA_DIR):\n",
    "        if filename.endswith(\"_comments.json\") or filename.endswith(\"_submissions.json\"):\n",
    "            input_file = os.path.join(RAW_DATA_DIR, filename)\n",
    "            output_file = os.path.join(PROCESSED_DATA_DIR, filename.replace(\".json\", \"_cleaned_model.json\"))\n",
    "            \n",
    "            with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "                for line in infile:\n",
    "                    data = json.loads(line)\n",
    "                    \n",
    "                    # Process body or selftext depending on file type\n",
    "                    text_field = \"selftext\" if \"selftext\" in data else \"body\"\n",
    "                    text = clean_text(data.get(text_field, \"\"))\n",
    "                    \n",
    "                    # Sentence tokenization\n",
    "                    sentences = sent_tokenize(text)\n",
    "                    \n",
    "                    # Word tokenization with stopword removal and lemmatization\n",
    "                    tokens = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text) if word.lower() not in STOPWORDS]\n",
    "                    \n",
    "                    # Store processed text\n",
    "                    data[\"sentences\"] = sentences\n",
    "                    data[\"tokens\"] = tokens\n",
    "                    \n",
    "                    json.dump(data, outfile)\n",
    "                    outfile.write(\"\\n\")\n",
    "            print(f\"Modelling preprocessing complete: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_for_modelling()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
